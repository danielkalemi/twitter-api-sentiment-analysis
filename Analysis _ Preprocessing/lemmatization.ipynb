{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc74b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e657f02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'capri_lynnn',\n",
       " 'the',\n",
       " 'email',\n",
       " 'i',\n",
       " 'sent',\n",
       " 'that',\n",
       " 'got',\n",
       " 'me',\n",
       " 'into',\n",
       " 'uc',\n",
       " 'berkeley',\n",
       " 'https',\n",
       " '//t.co/0ygizuabwmso',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'see',\n",
       " 'rachelegross',\n",
       " \"'s\",\n",
       " 'book',\n",
       " 'featured',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'york',\n",
       " 'times',\n",
       " 'today',\n",
       " 'i',\n",
       " 'still',\n",
       " 'look',\n",
       " 'back',\n",
       " 'fondly',\n",
       " 'at',\n",
       " 'our',\n",
       " 'time',\n",
       " 'at',\n",
       " 'uc',\n",
       " 'berkeley',\n",
       " 'a',\n",
       " 'decade',\n",
       " 'ago',\n",
       " 'https',\n",
       " '//t.co/hos7j7bhqurt',\n",
       " 'theblackchefzw',\n",
       " 'i',\n",
       " 'hold',\n",
       " 'bachelors',\n",
       " 'degrees',\n",
       " 'molecular',\n",
       " 'cellular',\n",
       " 'and',\n",
       " 'development',\n",
       " 'biology',\n",
       " 'csu',\n",
       " 'fresno',\n",
       " 'political',\n",
       " 'science',\n",
       " 'uc',\n",
       " 'ber…',\n",
       " 'k_t_chen',\n",
       " 'good',\n",
       " 'luck',\n",
       " 'and',\n",
       " 'other',\n",
       " 'best',\n",
       " 'wishes',\n",
       " 'on',\n",
       " 'a',\n",
       " 'more',\n",
       " 'positive',\n",
       " 'note',\n",
       " 'i',\n",
       " 'started',\n",
       " 'college',\n",
       " 'after',\n",
       " '2yrs',\n",
       " 'of',\n",
       " 'religious',\n",
       " 'training',\n",
       " 'my',\n",
       " 'frosh',\n",
       " 'english',\n",
       " 'prof',\n",
       " 'was',\n",
       " 'an',\n",
       " 'atheist',\n",
       " 'from',\n",
       " 'uc',\n",
       " 'berkeley',\n",
       " 'we',\n",
       " 'clashed',\n",
       " 'i',\n",
       " 'survived',\n",
       " 'hand',\n",
       " 'the',\n",
       " 'piece',\n",
       " 'in',\n",
       " 'and',\n",
       " 'write',\n",
       " 'it']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# opened our tweets and made it unified in lowercase\n",
    "with open('tweets.txt','r') as f:\n",
    "    tw_str = f.read().lower()\n",
    "    \n",
    "# tokenize blob of text\n",
    "tweet_words = nltk.word_tokenize(tw_str)\n",
    "\n",
    "# create a list of alphabetic tokens\n",
    "tweet_words = [word for word in tweet_words if any(l.isalpha() for l in word)]\n",
    "\n",
    "tweet_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168d2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stpwrds = stopwords.words('english')\n",
    "stpwrds.append('\\'s')\n",
    "        \n",
    "def get_clean_tokens(txt,stpwrds):\n",
    "    # make everything lowercase\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # tokenize blob of text\n",
    "    tweet_words = nltk.word_tokenize(txt)\n",
    "\n",
    "    # create a list of alphabetic tokens\n",
    "    tweet_words = [word for word in tweet_words if any(l.isalpha() for l in word)]\n",
    "\n",
    "    #remove stopwords\n",
    "    salient_words = [word for word in tweet_words if word not in stpwrds]\n",
    "    \n",
    "    return salient_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977bf6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danielkalemi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danielkalemi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc1f2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# # tagging each word with their grammatical role in a sentence\n",
    "# word_tags = pos_tag(salient_words)\n",
    "\n",
    "# # empty list to fill in with clean words\n",
    "# clean_tokens=[]\n",
    "\n",
    "# # instantite a lemmatizer object\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "# for token, tag in word_tags:\n",
    "#     # clean up tags\n",
    "#     if tag.startswith('NN'):\n",
    "#         pos = 'n'\n",
    "#     elif tag.startswith('VB'):\n",
    "#         pos = 'v'\n",
    "#     else:\n",
    "#         pos = 'a'\n",
    "    \n",
    "#     # perform lemmatization\n",
    "#     token = lemmatizer.lemmatize(token, pos)\n",
    "    \n",
    "#     # add the token to the clean_tokens list\n",
    "#     clean_tokens.append(token)\n",
    "    \n",
    "# clean_tokens[:100]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef2ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def normalize_tweet(tweet_tokens):\n",
    "    clean_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)','', token)\n",
    "        token = re.sub('(#[A-Za-z0-9_]+)','', token)\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        if (len(token)>0):\n",
    "            clean_tokens.append(token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587a43ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Save the date for April 10th. See all the details for this upcoming local event on Patch. https://t.co/U3qNnX5eS1',\n",
       " 'Save the date for April 10th. See all the details for this upcoming local event on Patch. https://t.co/fdjxuc7osb']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a list of tweets; each item on the list is a tweet\n",
    "import pickle\n",
    "\n",
    "with open('tweets.pickle','rb') as f:\n",
    "    tweets = pickle.load(f)\n",
    "    \n",
    "tweets[:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c961609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = []\n",
    "for t in tweets:\n",
    "    t = nltk.word_tokenize(t)\n",
    "    tw.append(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ebaf874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Save',\n",
       "  'the',\n",
       "  'date',\n",
       "  'for',\n",
       "  'April',\n",
       "  '10th',\n",
       "  '.',\n",
       "  'See',\n",
       "  'all',\n",
       "  'the',\n",
       "  'details',\n",
       "  'for',\n",
       "  'this',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'on',\n",
       "  'Patch',\n",
       "  '.',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/U3qNnX5eS1'],\n",
       " ['Save',\n",
       "  'the',\n",
       "  'date',\n",
       "  'for',\n",
       "  'April',\n",
       "  '10th',\n",
       "  '.',\n",
       "  'See',\n",
       "  'all',\n",
       "  'the',\n",
       "  'details',\n",
       "  'for',\n",
       "  'this',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'on',\n",
       "  'Patch',\n",
       "  '.',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/fdjxuc7osb']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d3dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#FollowFriday',\n",
       "  '@France_Inte',\n",
       "  '@PKuchly57',\n",
       "  '@Milipol_Paris',\n",
       "  'for',\n",
       "  'being',\n",
       "  'top',\n",
       "  'engaged',\n",
       "  'members',\n",
       "  'in',\n",
       "  'my',\n",
       "  'community',\n",
       "  'this',\n",
       "  'week',\n",
       "  ':)'],\n",
       " ['@Lamb2ja',\n",
       "  'Hey',\n",
       "  'James',\n",
       "  '!',\n",
       "  'How',\n",
       "  'odd',\n",
       "  ':/',\n",
       "  'Please',\n",
       "  'call',\n",
       "  'our',\n",
       "  'Contact',\n",
       "  'Centre',\n",
       "  'on',\n",
       "  '02392441234',\n",
       "  'and',\n",
       "  'we',\n",
       "  'will',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'assist',\n",
       "  'you',\n",
       "  ':)',\n",
       "  'Many',\n",
       "  'thanks',\n",
       "  '!'],\n",
       " ['@DespiteOfficial',\n",
       "  'we',\n",
       "  'had',\n",
       "  'a',\n",
       "  'listen',\n",
       "  'last',\n",
       "  'night',\n",
       "  ':)',\n",
       "  'As',\n",
       "  'You',\n",
       "  'Bleed',\n",
       "  'is',\n",
       "  'an',\n",
       "  'amazing',\n",
       "  'track',\n",
       "  '.',\n",
       "  'When',\n",
       "  'are',\n",
       "  'you',\n",
       "  'in',\n",
       "  'Scotland',\n",
       "  '?',\n",
       "  '!'],\n",
       " ['@97sides', 'CONGRATS', ':)'],\n",
       " ['yeaaaah',\n",
       "  'yippppy',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  'my',\n",
       "  'accnt',\n",
       "  'verified',\n",
       "  'rqst',\n",
       "  'has',\n",
       "  'succeed',\n",
       "  'got',\n",
       "  'a',\n",
       "  'blue',\n",
       "  'tick',\n",
       "  'mark',\n",
       "  'on',\n",
       "  'my',\n",
       "  'fb',\n",
       "  'profile',\n",
       "  ':)',\n",
       "  'in',\n",
       "  '15',\n",
       "  'days'],\n",
       " ['@BhaktisBanter',\n",
       "  '@PallaviRuhail',\n",
       "  'This',\n",
       "  'one',\n",
       "  'is',\n",
       "  'irresistible',\n",
       "  ':)',\n",
       "  '#FlipkartFashionFriday',\n",
       "  'http://t.co/EbZ0L2VENM'],\n",
       " ['We',\n",
       "  \"don't\",\n",
       "  'like',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'our',\n",
       "  'lovely',\n",
       "  'customers',\n",
       "  'waiting',\n",
       "  'for',\n",
       "  'long',\n",
       "  '!',\n",
       "  'We',\n",
       "  'hope',\n",
       "  'you',\n",
       "  'enjoy',\n",
       "  '!',\n",
       "  'Happy',\n",
       "  'Friday',\n",
       "  '!',\n",
       "  '-',\n",
       "  'LWWF',\n",
       "  ':)',\n",
       "  'https://t.co/smyYriipxI'],\n",
       " ['@Impatientraider',\n",
       "  'On',\n",
       "  'second',\n",
       "  'thought',\n",
       "  ',',\n",
       "  'there',\n",
       "  '’',\n",
       "  's',\n",
       "  'just',\n",
       "  'not',\n",
       "  'enough',\n",
       "  'time',\n",
       "  'for',\n",
       "  'a',\n",
       "  'DD',\n",
       "  ':)',\n",
       "  'But',\n",
       "  'new',\n",
       "  'shorts',\n",
       "  'entering',\n",
       "  'system',\n",
       "  '.',\n",
       "  'Sheep',\n",
       "  'must',\n",
       "  'be',\n",
       "  'buying',\n",
       "  '.'],\n",
       " ['Jgh', ',', 'but', 'we', 'have', 'to', 'go', 'to', 'Bayan', ':D', 'bye'],\n",
       " ['As',\n",
       "  'an',\n",
       "  'act',\n",
       "  'of',\n",
       "  'mischievousness',\n",
       "  ',',\n",
       "  'am',\n",
       "  'calling',\n",
       "  'the',\n",
       "  'ETL',\n",
       "  'layer',\n",
       "  'of',\n",
       "  'our',\n",
       "  'in-house',\n",
       "  'warehousing',\n",
       "  'app',\n",
       "  'Katamari',\n",
       "  '.',\n",
       "  'Well',\n",
       "  '…',\n",
       "  'as',\n",
       "  'the',\n",
       "  'name',\n",
       "  'implies',\n",
       "  ':p',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download sample tweets from NLTK\n",
    "# nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741062ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sample_tokens(tokens, stpwrds):\n",
    "    clean=[]\n",
    "    for t in tokens:\n",
    "        \n",
    "        cc = normalize_tweet(t)\n",
    "        \n",
    "        # get list of salient words\n",
    "        salient_words = [word for word in cc if (word not in stpwrds)]\n",
    "\n",
    "        sw = [w for w in salient_words if len(w)>1]\n",
    "\n",
    "        clean.append(sw)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904e825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = clean_sample_tokens(tw, stpwrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd6723b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Save',\n",
       "  'date',\n",
       "  'April',\n",
       "  '10th',\n",
       "  'See',\n",
       "  'detail',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'Patch',\n",
       "  'http',\n",
       "  '//t.co/U3qNnX5eS1'],\n",
       " ['Save',\n",
       "  'date',\n",
       "  'April',\n",
       "  '10th',\n",
       "  'See',\n",
       "  'detail',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'Patch',\n",
       "  'http',\n",
       "  '//t.co/fdjxuc7osb']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3eb4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Save',\n",
       "  'date',\n",
       "  'April',\n",
       "  '10th',\n",
       "  'See',\n",
       "  'detail',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'Patch',\n",
       "  'http',\n",
       "  '//t.co/U3qNnX5eS1'],\n",
       " ['Save',\n",
       "  'date',\n",
       "  'April',\n",
       "  '10th',\n",
       "  'See',\n",
       "  'detail',\n",
       "  'upcoming',\n",
       "  'local',\n",
       "  'event',\n",
       "  'Patch',\n",
       "  'http',\n",
       "  '//t.co/fdjxuc7osb'],\n",
       " ['Annie',\n",
       "  'get',\n",
       "  'two',\n",
       "  'husband',\n",
       "  'UC',\n",
       "  'Berkeley',\n",
       "  'peregrine',\n",
       "  'falcon',\n",
       "  'may',\n",
       "  'new',\n",
       "  'mate',\n",
       "  'one',\n",
       "  'day',\n",
       "  'Grinnell',\n",
       "  'death',\n",
       "  'http',\n",
       "  '//t.co/xCV2RzeHH8'],\n",
       " ['bareylena',\n",
       "  'exactly',\n",
       "  'like',\n",
       "  'register',\n",
       "  'democrat',\n",
       "  'nimby',\n",
       "  'sue',\n",
       "  'uc',\n",
       "  'berkeley',\n",
       "  'small',\n",
       "  'class',\n",
       "  'awful'],\n",
       " ['Hello',\n",
       "  'MedTwitter',\n",
       "  'soon-to-be',\n",
       "  'MD/MS',\n",
       "  'student',\n",
       "  'start',\n",
       "  'fall',\n",
       "  'UC',\n",
       "  'Berkeley-UCSF',\n",
       "  'Joint',\n",
       "  'Medical',\n",
       "  'Program',\n",
       "  'Interested',\n",
       "  'HarmReduction',\n",
       "  'HealthEquity',\n",
       "  'Hoping',\n",
       "  'HR',\n",
       "  'volunteering/research/advocacy',\n",
       "  'med',\n",
       "  'school',\n",
       "  'Will',\n",
       "  'tweet',\n",
       "  'HR',\n",
       "  'also',\n",
       "  'thing'],\n",
       " ['Join',\n",
       "  'amp',\n",
       "  'terrific',\n",
       "  'group',\n",
       "  'co-panelists',\n",
       "  'discuss',\n",
       "  'Social',\n",
       "  'amp',\n",
       "  'Economic',\n",
       "  'Impacts',\n",
       "  'Wildfires',\n",
       "  'Monday',\n",
       "  'April',\n",
       "  '4th',\n",
       "  '12:30',\n",
       "  'pm',\n",
       "  '1:45',\n",
       "  'pm',\n",
       "  'PT',\n",
       "  'Co-sponsored',\n",
       "  'BerkeleyLawCLEE',\n",
       "  'UC',\n",
       "  'Berkeley',\n",
       "  'Social',\n",
       "  'Science',\n",
       "  'Matrix',\n",
       "  'Registration',\n",
       "  'free',\n",
       "  'http',\n",
       "  '//t.co/MCbgkahbA6'],\n",
       " ['Easily',\n",
       "  'one',\n",
       "  'great',\n",
       "  'testimonial',\n",
       "  'witness',\n",
       "  'The',\n",
       "  'way',\n",
       "  'sell',\n",
       "  'ideal',\n",
       "  'student',\n",
       "  'UC',\n",
       "  'Berkeley',\n",
       "  'shock',\n",
       "  'drop',\n",
       "  'sell',\n",
       "  'product',\n",
       "  'online',\n",
       "  'Kudos',\n",
       "  'Capri',\n",
       "  'http',\n",
       "  '//t.co/uz3R6GCkgr'],\n",
       " ['RT',\n",
       "  'theblackchefzw',\n",
       "  'hold',\n",
       "  'bachelor',\n",
       "  'degree',\n",
       "  'Molecular',\n",
       "  'Cellular',\n",
       "  'Development',\n",
       "  'Biology',\n",
       "  'CSU',\n",
       "  'Fresno',\n",
       "  'Political',\n",
       "  'Science',\n",
       "  'UC',\n",
       "  'Ber…'],\n",
       " ['RT',\n",
       "  'Capri_lynnn',\n",
       "  'email',\n",
       "  'send',\n",
       "  'get',\n",
       "  'UC',\n",
       "  'Berkeley',\n",
       "  'http',\n",
       "  '//t.co/0ygizuABwM'],\n",
       " ['RT',\n",
       "  'Capri_lynnn',\n",
       "  'email',\n",
       "  'send',\n",
       "  'get',\n",
       "  'UC',\n",
       "  'Berkeley',\n",
       "  'http',\n",
       "  '//t.co/0ygizuABwM']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematized_tweet = []\n",
    "    \n",
    "for tl in tweets:\n",
    "    tl = normalize_tweet(tl)\n",
    "    lematized_tweet.append(tl)\n",
    "    \n",
    "lematized_tweet[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "032637e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = clean_sample_tokens(positive_tweets, stpwrds)\n",
    "negative_tweets = clean_sample_tokens(negative_tweets, stpwrds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca91b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['top', 'engage', 'member', 'community', 'week', ':)'],\n",
       " ['Hey',\n",
       "  'James',\n",
       "  'How',\n",
       "  'odd',\n",
       "  ':/',\n",
       "  'Please',\n",
       "  'call',\n",
       "  'Contact',\n",
       "  'Centre',\n",
       "  '02392441234',\n",
       "  'able',\n",
       "  'assist',\n",
       "  ':)',\n",
       "  'Many',\n",
       "  'thanks']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e23d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['top', 'engage', 'member', 'community', 'week', ':)'],\n",
       " ['Hey',\n",
       "  'James',\n",
       "  'How',\n",
       "  'odd',\n",
       "  ':/',\n",
       "  'Please',\n",
       "  'call',\n",
       "  'Contact',\n",
       "  'Centre',\n",
       "  '02392441234',\n",
       "  'able',\n",
       "  'assist',\n",
       "  ':)',\n",
       "  'Many',\n",
       "  'thanks'],\n",
       " ['listen',\n",
       "  'last',\n",
       "  'night',\n",
       "  ':)',\n",
       "  'As',\n",
       "  'You',\n",
       "  'Bleed',\n",
       "  'amaze',\n",
       "  'track',\n",
       "  'When',\n",
       "  'Scotland'],\n",
       " ['CONGRATS', ':)'],\n",
       " ['yeaaaah',\n",
       "  'yippppy',\n",
       "  'accnt',\n",
       "  'verify',\n",
       "  'rqst',\n",
       "  'succeed',\n",
       "  'get',\n",
       "  'blue',\n",
       "  'tick',\n",
       "  'mark',\n",
       "  'fb',\n",
       "  'profile',\n",
       "  ':)',\n",
       "  '15',\n",
       "  'day'],\n",
       " ['This', 'one', 'irresistible', ':)'],\n",
       " ['We',\n",
       "  'like',\n",
       "  'keep',\n",
       "  'lovely',\n",
       "  'customer',\n",
       "  'wait',\n",
       "  'long',\n",
       "  'We',\n",
       "  'hope',\n",
       "  'enjoy',\n",
       "  'Happy',\n",
       "  'Friday',\n",
       "  'LWWF',\n",
       "  ':)'],\n",
       " ['On',\n",
       "  'second',\n",
       "  'think',\n",
       "  'enough',\n",
       "  'time',\n",
       "  'DD',\n",
       "  ':)',\n",
       "  'But',\n",
       "  'new',\n",
       "  'short',\n",
       "  'enter',\n",
       "  'system',\n",
       "  'Sheep',\n",
       "  'must',\n",
       "  'buy'],\n",
       " ['Jgh', 'go', 'Bayan', ':D', 'bye'],\n",
       " ['As',\n",
       "  'act',\n",
       "  'mischievousness',\n",
       "  'call',\n",
       "  'ETL',\n",
       "  'layer',\n",
       "  'in-house',\n",
       "  'warehouse',\n",
       "  'app',\n",
       "  'Katamari',\n",
       "  'Well',\n",
       "  'name',\n",
       "  'imply',\n",
       "  ':p']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematized_p_tweet = []\n",
    "\n",
    "    \n",
    "for tl in positive_tweets:\n",
    "    tl = normalize_tweet(tl)\n",
    "    lematized_p_tweet.append(tl)\n",
    "    \n",
    "lematized_p_tweet[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cc6f1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hopeless', 'tmr', ':('],\n",
       " ['Everything',\n",
       "  'kid',\n",
       "  'section',\n",
       "  'IKEA',\n",
       "  'cute',\n",
       "  'Shame',\n",
       "  \"I'm\",\n",
       "  'nearly',\n",
       "  '19',\n",
       "  'month',\n",
       "  ':('],\n",
       " ['That', 'heart', 'slide', 'waste', 'basket', ':('],\n",
       " ['hate', 'Japanese', 'call', 'ban', ':(', ':(', 'Me'],\n",
       " ['Dang', 'start', 'next', 'week', 'work', ':('],\n",
       " ['oh', 'god', 'baby', 'face', ':('],\n",
       " ['make', 'smile', ':('],\n",
       " ['work',\n",
       "  'neighbour',\n",
       "  'motor',\n",
       "  'Asked',\n",
       "  'say',\n",
       "  'hat',\n",
       "  'update',\n",
       "  'search',\n",
       "  ':('],\n",
       " [':(', 'sialan', ':('],\n",
       " ['Athabasca', 'glacier', ':-(']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematized_n_tweet = []\n",
    "\n",
    "    \n",
    "for tl in negative_tweets:\n",
    "    tl = normalize_tweet(tl)\n",
    "    lematized_n_tweet.append(tl)\n",
    "    \n",
    "lematized_n_tweet[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbc17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build ML model\n",
    "\n",
    "def generate_token_dict (tokens_list):\n",
    "    for tokens in tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9018138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our positive and negative tweet tokens for the model\n",
    "pos_model_dict = generate_token_dict(lematized_p_tweet)\n",
    "neg_model_dict = generate_token_dict(lematized_n_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2796a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for training our model i.e. a NaiveBayesClassifier\n",
    "pos_dataset = [(pdict,'positive') for pdict in pos_model_dict]\n",
    "neg_dataset = [(pdict,'negative') for pdict in neg_model_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a9fa7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'Hi': True,\n",
       "   'BAM': True,\n",
       "   'Can': True,\n",
       "   'follow': True,\n",
       "   'bestfriend': True,\n",
       "   'She': True,\n",
       "   'love': True,\n",
       "   'lot': True,\n",
       "   ':)': True,\n",
       "   'See': True,\n",
       "   'Warsaw': True,\n",
       "   '<3': True,\n",
       "   'Love': True,\n",
       "   'x32': True},\n",
       "  'positive'),\n",
       " ({'still': True, 'want': True, 'cactus': True, ':(': True}, 'negative'),\n",
       " ({'yo': True, 'yall': True, 'invite': True, 'rank': True, ':)': True},\n",
       "  'positive'),\n",
       " ({'Can': True,\n",
       "   'please': True,\n",
       "   'woes': True,\n",
       "   \"can't\": True,\n",
       "   'run': True,\n",
       "   'six': True,\n",
       "   'though': True,\n",
       "   'live': True,\n",
       "   'Mexico': True,\n",
       "   ':(': True},\n",
       "  'negative')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "fullset = pos_dataset + neg_dataset\n",
    "\n",
    "random.shuffle(fullset)\n",
    "fullset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7adcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = round(len(fullset)*0.7)\n",
    "trainset = fullset[:cutoff]\n",
    "testset = fullset[cutoff:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
